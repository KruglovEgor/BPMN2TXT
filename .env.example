# Общие переменные для docker-compose.yml в корне репозитория

# Путь до модели внутри контейнера (volume монтируется в /models)
MODEL_PATH=/models/qwen2.5-7b-instruct-q4_k_m.gguf

# Ссылка на GGUF (используется model-init для скачивания)
HF_MODEL_URL=https://huggingface.co/paultimothymooney/Qwen2.5-7B-Instruct-Q4_K_M-GGUF/resolve/main/qwen2.5-7b-instruct-q4_k_m.gguf

# Порты для CPU и GPU серверов llama.cpp
CPU_HOST_PORT=8000
GPU_HOST_PORT=8000

# Размер окна ответа модели
N_PREDICT=4096

# CPU параметры
CPU_CTX_SIZE=4096
CPU_THREADS=6
CPU_EXTRA_ARGS=

# GPU параметры
GPU_CTX_SIZE=4096
GPU_THREADS=6
N_GPU_LAYERS=999
GPU_EXTRA_ARGS=
