# Общие переменные для docker-compose.yml в корне репозитория

# Путь до модели внутри контейнера (volume монтируется в /models)
MODEL_PATH=/models/qwen2.5-7b-instruct-q4_k_m.gguf

# Ссылка на GGUF (используется model-init для скачивания)
HF_MODEL_URL=https://huggingface.co/paultimothymooney/Qwen2.5-7B-Instruct-Q4_K_M-GGUF/resolve/main/qwen2.5-7b-instruct-q4_k_m.gguf

# Порты для CPU и GPU серверов llama.cpp
CPU_HOST_PORT=8000
GPU_HOST_PORT=8001

# Размер окна ответа модели
N_PREDICT=4096

# CPU параметры
CPU_CTX_SIZE=4096
CPU_THREADS=6
CPU_EXTRA_ARGS=

# GPU параметры
GPU_CTX_SIZE=4096
GPU_THREADS=6
N_GPU_LAYERS=999
GPU_EXTRA_ARGS=

# Gateway service settings
GATEWAY_PORT=7000
UI_PORT=3000
PIPELINE_URL=http://pipeline:5000
LLM_URL=http://llm-gpu:8001
LLM_MODEL=local-model
LLM_TEMPERATURE=0.2
LLM_MAX_TOKENS=4096
HTTP_TIMEOUT=60
PROMPT_BPMN_TO_TEXT=Тебе дан JSON с извлечением BPMN. Он может быть шумным или неполным. Напиши краткое, понятное описание процесса по шагам. Не упоминай JSON.\n\nJSON:\n{payload}\n
PROMPT_TEXT_TO_MERMAID=Конвертируй следующее описание процесса в код диаграммы Mermaid. Возвращай только код Mermaid, без ограждающих символов или объяснений.\n\nОписание:\n{description}\n
